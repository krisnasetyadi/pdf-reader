{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a82cab7f",
   "metadata": {},
   "source": [
    "# ğŸ” DocuLens - Hybrid Document QA System\n",
    "\n",
    "**DocuLens** adalah sistem hybrid search yang memungkinkan Anda bertanya tentang dokumen PDF, database, dan chat logs menggunakan bahasa natural.\n",
    "\n",
    "## âœ¨ Fitur Utama:\n",
    "- ğŸ“„ **PDF Processing** - Upload dan proses multiple PDF files\n",
    "- ğŸ” **Semantic Search** - Pencarian berbasis meaning dengan query expansion\n",
    "- ğŸ¤– **AI Answer Generation** - Jawaban otomatis dari konteks dokumen\n",
    "- ğŸ—„ï¸ **Database Integration** - Query ke SQLite/PostgreSQL\n",
    "- ğŸ’¬ **Chat History Search** - Cari dari riwayat chat (WhatsApp, Telegram, dll)\n",
    "- ğŸ”„ **Hybrid Search** - Kombinasi semua sumber data\n",
    "- ğŸ‡®ğŸ‡© **Indonesian NLP** - Stemming & query expansion untuk Bahasa Indonesia\n",
    "\n",
    "## ğŸ“Š Flow End-to-End:\n",
    "```\n",
    "1. Setup & Install â†’ 2. Load Models â†’ 3. Process PDF â†’ 4. Create Vector Store\n",
    "     â†“                                                          â†“\n",
    "5. Setup Database (Optional) â†’ 6. Hybrid Search â†’ 7. Generate Answer\n",
    "```\n",
    "\n",
    "---\n",
    "**Author:** DocuLens Team  \n",
    "**Runtime:** GPU (T4) recommended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82ea708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all dependencies (ignore version warnings - they're harmless)\n",
    "!pip install -q transformers torch sentence-transformers langchain langchain-community langchain-huggingface langchain-text-splitters faiss-cpu pdfplumber accelerate sqlalchemy 2>/dev/null\n",
    "\n",
    "print(\"âœ… Dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c47ff2",
   "metadata": {},
   "source": [
    "## 2ï¸âƒ£ Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65ec273",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import sqlite3\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, pipeline, GenerationConfig\n",
    "from langchain_huggingface import HuggingFacePipeline, HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "import pdfplumber\n",
    "\n",
    "# Check device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"ğŸ–¥ï¸ Using device: {device}\")\n",
    "if device == \"cuda\":\n",
    "    print(f\"ğŸ® GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"ğŸ’¾ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "# Define Search Type Enum\n",
    "class SearchType(Enum):\n",
    "    UNSTRUCTURED = \"unstructured\"  # PDF only\n",
    "    STRUCTURED = \"structured\"      # Database only\n",
    "    HYBRID = \"hybrid\"              # Both PDF + Database + Chat\n",
    "\n",
    "print(\"âœ… Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f0e130",
   "metadata": {},
   "source": [
    "## 3ï¸âƒ£ Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb38bbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    # Model settings - GRATIS!\n",
    "    MODEL_NAME: str = \"google/flan-t5-base\"  # Options: flan-t5-small, flan-t5-base, flan-t5-large\n",
    "    EMBEDDING_MODEL: str = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "    \n",
    "    # Chunk settings for PDF processing\n",
    "    CHUNK_SIZE: int = 500\n",
    "    CHUNK_OVERLAP: int = 50\n",
    "    \n",
    "    # Search settings\n",
    "    TOP_K: int = 5  # Results per source\n",
    "    K_PER_COLLECTION: int = 3\n",
    "    SIMILARITY_THRESHOLD: float = 0.3\n",
    "    \n",
    "    # Generation settings\n",
    "    MAX_NEW_TOKENS: int = 256\n",
    "    TEMPERATURE: float = 0.3\n",
    "    \n",
    "    # Query Expansion Terms (Indonesian + English)\n",
    "    QUERY_EXPANSION_TERMS: Dict = None\n",
    "    \n",
    "    # Database keywords for routing\n",
    "    DB_KEYWORDS: List = None\n",
    "    \n",
    "    # PDF keywords for routing\n",
    "    PDF_KEYWORDS: List = None\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self.QUERY_EXPANSION_TERMS = {\n",
    "            \"apa itu\": [\"definisi\", \"pengertian\", \"arti\", \"makna\", \"jelaskan\"],\n",
    "            \"proses\": [\"tahapan\", \"langkah\", \"mekanisme\", \"cara kerja\"],\n",
    "            \"user\": [\"pengguna\", \"karyawan\", \"staff\", \"employee\", \"profil\"],\n",
    "            \"product\": [\"produk\", \"barang\", \"item\", \"inventory\"],\n",
    "            \"order\": [\"pesanan\", \"pembelian\", \"transaksi\", \"pemesanan\"],\n",
    "        }\n",
    "        \n",
    "        self.DB_KEYWORDS = [\n",
    "            'user', 'profile', 'customer', 'product', 'order', 'price', \n",
    "            'jumlah', 'total', 'data', 'tabel', 'table', 'database', 'sql',\n",
    "            'nama', 'email', 'alamat', 'tanggal', 'harga', 'stock',\n",
    "            'karyawan', 'transaksi', 'pesanan', 'siapa', 'pegawai'\n",
    "        ]\n",
    "        \n",
    "        self.PDF_KEYWORDS = [\n",
    "            'dokumen', 'pdf', 'file', 'laporan', 'report', 'handbook',\n",
    "            'kebijakan', 'policy', 'prosedur', 'pedoman', 'guideline',\n",
    "            'kontrak', 'agreement', 'proposal', 'fungsional', 'persyaratan'\n",
    "        ]\n",
    "\n",
    "config = Config()\n",
    "print(\"âœ… Configuration loaded\")\n",
    "print(f\"   ğŸ“¦ LLM Model: {config.MODEL_NAME}\")\n",
    "print(f\"   ğŸ“¦ Embedding: {config.EMBEDDING_MODEL}\")\n",
    "print(f\"   ğŸ“Š Chunk Size: {config.CHUNK_SIZE}\")\n",
    "print(f\"   ğŸ” Top K: {config.TOP_K}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d269ff58",
   "metadata": {},
   "source": [
    "## 4ï¸âƒ£ Initialize Models\n",
    "\n",
    "Load LLM dan Embedding models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f711bd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ”„ Loading LLM model...\")\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.MODEL_NAME)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    config.MODEL_NAME,\n",
    "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32\n",
    ")\n",
    "\n",
    "# Move model to device\n",
    "if device == \"cuda\":\n",
    "    model = model.to(device)\n",
    "\n",
    "# Create pipeline - simplified without generation_config (compatible with latest transformers)\n",
    "pipe = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=config.MAX_NEW_TOKENS,\n",
    "    temperature=config.TEMPERATURE,\n",
    "    do_sample=True,\n",
    "    device=0 if device == \"cuda\" else -1\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "print(f\"âœ… LLM loaded: {config.MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab3ce6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ”„ Loading embedding model...\")\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=config.EMBEDDING_MODEL,\n",
    "    model_kwargs={'device': device},\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")\n",
    "\n",
    "print(f\"âœ… Embeddings loaded: {config.EMBEDDING_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2813599f",
   "metadata": {},
   "source": [
    "## 5ï¸âƒ£ PDF Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da94ed79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== PDF EXTRACTION =====\n",
    "def extract_text_from_pdf(pdf_path: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Extract text from PDF file with page information.\"\"\"\n",
    "    pages_data = []\n",
    "    \n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page_num, page in enumerate(pdf.pages, start=1):\n",
    "                text = page.extract_text() or \"\"\n",
    "                text = re.sub(r'\\s+', ' ', text).strip()\n",
    "                \n",
    "                if text:\n",
    "                    pages_data.append({\n",
    "                        'text': text,\n",
    "                        'page': page_num,\n",
    "                        'source': os.path.basename(pdf_path)\n",
    "                    })\n",
    "                    \n",
    "        print(f\"ğŸ“„ Extracted {len(pages_data)} pages from {os.path.basename(pdf_path)}\")\n",
    "        return pages_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error extracting PDF: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "# ===== DOCUMENT CHUNKING =====\n",
    "def create_documents(pages_data: List[Dict]) -> List[Document]:\n",
    "    \"\"\"Create LangChain documents from extracted pages.\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=config.CHUNK_SIZE,\n",
    "        chunk_overlap=config.CHUNK_OVERLAP,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "    )\n",
    "    \n",
    "    documents = []\n",
    "    for page_data in pages_data:\n",
    "        chunks = text_splitter.split_text(page_data['text'])\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            doc = Document(\n",
    "                page_content=chunk,\n",
    "                metadata={\n",
    "                    'source': page_data['source'],\n",
    "                    'page': page_data['page']\n",
    "                }\n",
    "            )\n",
    "            documents.append(doc)\n",
    "    \n",
    "    print(f\"ğŸ“ Created {len(documents)} document chunks\")\n",
    "    return documents\n",
    "\n",
    "\n",
    "# ===== VECTOR STORE =====\n",
    "def create_vector_store(documents: List[Document], embeddings) -> FAISS:\n",
    "    \"\"\"Create FAISS vector store from documents.\"\"\"\n",
    "    print(\"ğŸ”„ Creating vector embeddings...\")\n",
    "    vector_store = FAISS.from_documents(documents, embeddings)\n",
    "    print(f\"âœ… Vector store created with {len(documents)} documents\")\n",
    "    return vector_store\n",
    "\n",
    "\n",
    "# ===== QUERY EXPANSION (Indonesian + English) =====\n",
    "def expand_query(query: str) -> List[str]:\n",
    "    \"\"\"Expand query with synonyms for better recall.\"\"\"\n",
    "    expanded_queries = [query, query.lower()]\n",
    "    \n",
    "    for term, synonyms in config.QUERY_EXPANSION_TERMS.items():\n",
    "        if term in query.lower():\n",
    "            for synonym in synonyms:\n",
    "                expanded_query = query.lower().replace(term, synonym)\n",
    "                expanded_queries.append(expanded_query)\n",
    "    \n",
    "    return list(set(expanded_queries))\n",
    "\n",
    "\n",
    "# ===== SIMPLE INDONESIAN STEMMING =====\n",
    "def simple_indonesian_stem(word: str) -> str:\n",
    "    \"\"\"Simple Indonesian stemming - remove common affixes.\"\"\"\n",
    "    word = word.lower().strip()\n",
    "    \n",
    "    suffixes = ['kan', 'an', 'i', 'nya', 'lah', 'kah']\n",
    "    prefixes = ['me', 'di', 'ke', 'se', 'ber', 'ter', 'pe']\n",
    "    \n",
    "    for suffix in suffixes:\n",
    "        if word.endswith(suffix) and len(word) > len(suffix) + 2:\n",
    "            word = word[:-len(suffix)]\n",
    "            break\n",
    "    \n",
    "    for prefix in prefixes:\n",
    "        if word.startswith(prefix) and len(word) > len(prefix) + 2:\n",
    "            word = word[len(prefix):]\n",
    "            break\n",
    "    \n",
    "    return word\n",
    "\n",
    "\n",
    "print(\"âœ… PDF processing & NLP functions loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10018797",
   "metadata": {},
   "source": [
    "## 6ï¸âƒ£ Search & Answer Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52cad10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== TEXT CLEANING =====\n",
    "def clean_context(text: str) -> str:\n",
    "    \"\"\"Clean context text from PDF artifacts.\"\"\"\n",
    "    text = re.sub(r'[â†’â†â†‘â†“â–ºâ—„â–²â–¼â– â–¡â—â—‹â—†â—‡]', '', text)\n",
    "    text = re.sub(r'[\\|â”‚â”ƒâ”†â”Šâ•]', ' ', text)\n",
    "    text = re.sub(r'[-â”€â”]{3,}', ' ', text)\n",
    "    text = re.sub(r'Page \\d+ of \\d+', '', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "# ===== QUESTION TYPE ANALYSIS =====\n",
    "def analyze_question_type(question: str) -> Dict[str, Any]:\n",
    "    \"\"\"Analyze question to determine optimal search strategy.\"\"\"\n",
    "    question_lower = question.lower()\n",
    "    \n",
    "    is_db_question = any(kw in question_lower for kw in config.DB_KEYWORDS)\n",
    "    is_pdf_question = any(kw in question_lower for kw in config.PDF_KEYWORDS)\n",
    "    \n",
    "    # Determine search type\n",
    "    if (is_db_question and is_pdf_question) or (not is_db_question and not is_pdf_question):\n",
    "        recommended_type = SearchType.HYBRID\n",
    "    elif is_db_question:\n",
    "        recommended_type = SearchType.STRUCTURED\n",
    "    else:\n",
    "        recommended_type = SearchType.UNSTRUCTURED\n",
    "    \n",
    "    # Extract search terms\n",
    "    stop_words = {'apa', 'siapa', 'dimana', 'kapan', 'berapa', 'bagaimana', \n",
    "                  'yang', 'dan', 'atau', 'di', 'ke', 'dari', 'dalam', 'pada', 'untuk'}\n",
    "    words = re.findall(r'\\b\\w+\\b', question_lower)\n",
    "    search_terms = [w for w in words if w not in stop_words and len(w) > 2]\n",
    "    \n",
    "    return {\n",
    "        \"recommended_type\": recommended_type,\n",
    "        \"is_db_question\": is_db_question,\n",
    "        \"is_pdf_question\": is_pdf_question,\n",
    "        \"search_terms\": search_terms\n",
    "    }\n",
    "\n",
    "\n",
    "# ===== PDF SEARCH WITH QUERY EXPANSION =====\n",
    "def search_documents(vector_store: FAISS, question: str, top_k: int = 5) -> List[Document]:\n",
    "    \"\"\"Search documents with query expansion.\"\"\"\n",
    "    expanded_queries = expand_query(question)\n",
    "    print(f\"ğŸ” Expanded queries: {expanded_queries[:3]}...\")\n",
    "    \n",
    "    all_results = []\n",
    "    seen_hashes = set()\n",
    "    \n",
    "    for eq in expanded_queries:\n",
    "        try:\n",
    "            results = vector_store.similarity_search_with_relevance_scores(eq, k=top_k)\n",
    "            \n",
    "            for doc, score in results:\n",
    "                if score >= config.SIMILARITY_THRESHOLD:\n",
    "                    content_hash = hash(doc.page_content[:100])\n",
    "                    if content_hash not in seen_hashes:\n",
    "                        seen_hashes.add(content_hash)\n",
    "                        doc.metadata['similarity_score'] = float(score)\n",
    "                        all_results.append((doc, score))\n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    # Sort and return top results\n",
    "    all_results.sort(key=lambda x: x[1], reverse=True)\n",
    "    filtered_docs = [doc for doc, _ in all_results[:top_k]]\n",
    "    \n",
    "    print(f\"ğŸ“š Found {len(filtered_docs)} relevant documents\")\n",
    "    return filtered_docs\n",
    "\n",
    "\n",
    "# ===== ANSWER GENERATION =====\n",
    "def generate_answer(llm, documents: List[Document], question: str, \n",
    "                    db_results: Dict = None, chat_docs: List = None) -> str:\n",
    "    \"\"\"Generate answer from hybrid sources.\"\"\"\n",
    "    if not documents and not db_results and not chat_docs:\n",
    "        return \"Maaf, tidak menemukan informasi yang relevan.\"\n",
    "    \n",
    "    context_parts = []\n",
    "    \n",
    "    # Add PDF context\n",
    "    if documents:\n",
    "        context_parts.append(\"=== DARI DOKUMEN PDF ===\")\n",
    "        for doc in documents[:3]:\n",
    "            source = doc.metadata.get('source', 'Unknown')\n",
    "            page = doc.metadata.get('page', '?')\n",
    "            content = clean_context(doc.page_content)[:300]\n",
    "            context_parts.append(f\"[{source}, Halaman {page}]\\n{content}\")\n",
    "    \n",
    "    # Add Database context\n",
    "    if db_results:\n",
    "        context_parts.append(\"\\n=== DARI DATABASE ===\")\n",
    "        for table, rows in db_results.items():\n",
    "            if rows:\n",
    "                context_parts.append(f\"Tabel {table}: {len(rows)} hasil ditemukan\")\n",
    "                for row in rows[:3]:\n",
    "                    context_parts.append(str(row))\n",
    "    \n",
    "    # Add Chat context\n",
    "    if chat_docs:\n",
    "        context_parts.append(\"\\n=== DARI CHAT HISTORY ===\")\n",
    "        for doc in chat_docs[:2]:\n",
    "            context_parts.append(f\"[Chat] {doc.page_content[:200]}\")\n",
    "    \n",
    "    context = \"\\n\\n\".join(context_parts)\n",
    "    \n",
    "    prompt = f\"\"\"Jawab pertanyaan berdasarkan konteks berikut. Jawab dalam Bahasa Indonesia.\n",
    "\n",
    "Konteks:\n",
    "{context}\n",
    "\n",
    "Pertanyaan: {question}\n",
    "\n",
    "Jawaban:\"\"\"\n",
    "    \n",
    "    try:\n",
    "        answer = llm.invoke(prompt)\n",
    "        return answer.strip()\n",
    "    except Exception as e:\n",
    "        return f\"Error generating answer: {str(e)}\"\n",
    "\n",
    "\n",
    "print(\"âœ… Search & Answer generation functions loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3bdf28",
   "metadata": {},
   "source": [
    "## 7ï¸âƒ£ Database Integration (Optional)\n",
    "\n",
    "Buat SQLite database untuk demo hybrid search. Skip jika hanya ingin PDF search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b60e352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== DATABASE MANAGER =====\n",
    "class DatabaseManager:\n",
    "    def __init__(self, db_path: str = \":memory:\"):\n",
    "        self.db_path = db_path\n",
    "        self.conn = None\n",
    "        \n",
    "    def connect(self):\n",
    "        self.conn = sqlite3.connect(self.db_path)\n",
    "        self.conn.row_factory = sqlite3.Row\n",
    "        print(f\"âœ… Connected to database: {self.db_path}\")\n",
    "        \n",
    "    def create_sample_data(self):\n",
    "        \"\"\"Create sample tables aligned with current Neon schema (user_profiles, products, orders).\"\"\"\n",
    "        cursor = self.conn.cursor()\n",
    "        \n",
    "        # ===== Table 1: user_profiles =====\n",
    "        cursor.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS user_profiles (\n",
    "                id INTEGER PRIMARY KEY,\n",
    "                name TEXT NOT NULL,\n",
    "                email TEXT UNIQUE NOT NULL,\n",
    "                department TEXT,\n",
    "                position TEXT,\n",
    "                phone TEXT,\n",
    "                created_at TEXT\n",
    "            )\n",
    "        ''')\n",
    "        \n",
    "        user_profiles_data = [\n",
    "            (1, 'Ahmad Wijaya', 'ahmad.wijaya@company.com', 'IT', 'Software Engineer', '+62-812-3456-7890', '2024-01-10'),\n",
    "            (2, 'Sari Dewi', 'sari.dewi@company.com', 'HR', 'HR Manager', '+62-813-4567-8901', '2024-01-11'),\n",
    "            (3, 'Budi Santoso', 'budi.santoso@company.com', 'Finance', 'Finance Analyst', '+62-814-5678-9012', '2024-01-12'),\n",
    "            (4, 'Maya Sari', 'maya.sari@company.com', 'Marketing', 'Marketing Specialist', '+62-815-6789-0123', '2024-01-13'),\n",
    "            (5, 'Rizki Pratama', 'rizki.pratama@company.com', 'IT', 'System Administrator', '+62-816-7890-1234', '2024-01-14'),\n",
    "        ]\n",
    "        cursor.executemany('INSERT OR REPLACE INTO user_profiles VALUES (?,?,?,?,?,?,?)', user_profiles_data)\n",
    "        \n",
    "        # ===== Table 2: products =====\n",
    "        cursor.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS products (\n",
    "                id INTEGER PRIMARY KEY,\n",
    "                name TEXT NOT NULL,\n",
    "                category TEXT,\n",
    "                price REAL,\n",
    "                description TEXT,\n",
    "                stock_quantity INTEGER,\n",
    "                created_at TEXT\n",
    "            )\n",
    "        ''')\n",
    "        \n",
    "        products_data = [\n",
    "            (1, 'Laptop ThinkPad X1', 'Electronics', 15000000, 'Business laptop dengan processor Intel i7 dan RAM 16GB', 25, '2024-01-10'),\n",
    "            (2, 'Smartphone Galaxy S23', 'Electronics', 12000000, 'Flagship smartphone dengan kamera 108MP', 50, '2024-01-11'),\n",
    "            (3, 'Office Chair Ergonomic', 'Furniture', 2500000, 'Kursi kantor ergonomis dengan lumbar support', 15, '2024-01-12'),\n",
    "            (4, 'Project Management Software', 'Software', 5000000, 'Software manajemen proyek dengan fitur kolaborasi tim', 100, '2024-01-13'),\n",
    "            (5, 'Wireless Mouse', 'Electronics', 350000, 'Mouse nirkabel dengan precision sensor', 75, '2024-01-14'),\n",
    "        ]\n",
    "        cursor.executemany('INSERT OR REPLACE INTO products VALUES (?,?,?,?,?,?,?)', products_data)\n",
    "        \n",
    "        # ===== Table 3: orders =====\n",
    "        cursor.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS orders (\n",
    "                id INTEGER PRIMARY KEY,\n",
    "                user_id INTEGER,\n",
    "                product_id INTEGER,\n",
    "                quantity INTEGER,\n",
    "                total_amount REAL,\n",
    "                status TEXT,\n",
    "                order_date TEXT,\n",
    "                created_at TEXT,\n",
    "                FOREIGN KEY(user_id) REFERENCES user_profiles(id),\n",
    "                FOREIGN KEY(product_id) REFERENCES products(id)\n",
    "            )\n",
    "        ''')\n",
    "        \n",
    "        orders_data = [\n",
    "            (1, 1, 1, 1, 15000000, 'completed', '2024-01-15', '2024-01-15'),\n",
    "            (2, 2, 3, 2, 5000000, 'completed', '2024-01-16', '2024-01-16'),\n",
    "            (3, 3, 2, 1, 12000000, 'pending', '2024-01-17', '2024-01-17'),\n",
    "            (4, 1, 4, 1, 5000000, 'completed', '2024-01-18', '2024-01-18'),\n",
    "            (5, 4, 5, 5, 1750000, 'shipped', '2024-01-19', '2024-01-19'),\n",
    "        ]\n",
    "        cursor.executemany('INSERT OR REPLACE INTO orders VALUES (?,?,?,?,?,?,?,?)', orders_data)\n",
    "        \n",
    "        self.conn.commit()\n",
    "        print(\"âœ… Sample database created with tables aligned to current schema:\")\n",
    "        print(\"   ğŸ“‹ user_profiles - Data karyawan/user\")\n",
    "        print(\"   ğŸ“‹ products - Data produk\")\n",
    "        print(\"   ğŸ“‹ orders - Data pesanan/transaksi\")\n",
    "        \n",
    "    def query(self, search_terms: List[str], tables: List[str] = None) -> Dict:\n",
    "        \"\"\"Search database tables.\"\"\"\n",
    "        if not self.conn:\n",
    "            return {}\n",
    "            \n",
    "        cursor = self.conn.cursor()\n",
    "        results = {}\n",
    "        \n",
    "        # Get all tables if not specified\n",
    "        if not tables:\n",
    "            cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\n",
    "            tables = [row[0] for row in cursor.fetchall()]\n",
    "        \n",
    "        for table in tables:\n",
    "            try:\n",
    "                # Get column names\n",
    "                cursor.execute(f\"PRAGMA table_info({table})\")\n",
    "                columns = [col[1] for col in cursor.fetchall()]\n",
    "                \n",
    "                # Build search query\n",
    "                conditions = []\n",
    "                for term in search_terms:\n",
    "                    for col in columns:\n",
    "                        conditions.append(f\"{col} LIKE '%{term}%'\")\n",
    "                \n",
    "                if conditions:\n",
    "                    query = f\"SELECT * FROM {table} WHERE {' OR '.join(conditions)} LIMIT 10\"\n",
    "                    cursor.execute(query)\n",
    "                    rows = [dict(row) for row in cursor.fetchall()]\n",
    "                    if rows:\n",
    "                        results[table] = rows\n",
    "            except Exception as e:\n",
    "                continue\n",
    "                \n",
    "        return results\n",
    "    \n",
    "    def get_all_tables(self) -> List[str]:\n",
    "        cursor = self.conn.cursor()\n",
    "        cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\n",
    "        return [row[0] for row in cursor.fetchall()]\n",
    "\n",
    "# Initialize database\n",
    "db_manager = DatabaseManager()\n",
    "db_manager.connect()\n",
    "db_manager.create_sample_data()\n",
    "\n",
    "print(f\"\\nğŸ“Š Available tables: {db_manager.get_all_tables()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac36e78",
   "metadata": {},
   "source": [
    "## 8ï¸âƒ£ Load PDF Files\n",
    "\n",
    "Letakkan file PDF di folder yang sama dengan notebook, atau gunakan sample PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e63d955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== OPTION 1: Use sample PDF from URL =====\n",
    "import urllib.request\n",
    "\n",
    "# Download sample PDF (replace with your own URL if needed)\n",
    "SAMPLE_PDF_URL = \"https://www.w3.org/WAI/WCAG21/Techniques/pdf/img/table-word.pdf\"\n",
    "PDF_FOLDER = \"pdf_files\"\n",
    "\n",
    "os.makedirs(PDF_FOLDER, exist_ok=True)\n",
    "\n",
    "# Download sample if no PDFs exist\n",
    "pdf_files = [f for f in os.listdir(PDF_FOLDER) if f.endswith('.pdf')] if os.path.exists(PDF_FOLDER) else []\n",
    "\n",
    "if not pdf_files:\n",
    "    print(\"ğŸ“¥ Downloading sample PDF...\")\n",
    "    sample_path = os.path.join(PDF_FOLDER, \"sample_document.pdf\")\n",
    "    try:\n",
    "        urllib.request.urlretrieve(SAMPLE_PDF_URL, sample_path)\n",
    "        pdf_files = [sample_path]\n",
    "        print(f\"âœ… Downloaded: {sample_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Could not download sample PDF: {e}\")\n",
    "        print(\"   Please add PDF files manually to 'pdf_files' folder\")\n",
    "else:\n",
    "    pdf_files = [os.path.join(PDF_FOLDER, f) for f in pdf_files]\n",
    "    print(f\"ğŸ“‚ Found {len(pdf_files)} PDF files in '{PDF_FOLDER}/'\")\n",
    "\n",
    "print(f\"\\nğŸ“„ PDF Files to process:\")\n",
    "for f in pdf_files:\n",
    "    print(f\"   - {os.path.basename(f)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae43359",
   "metadata": {},
   "source": [
    "## 9ï¸âƒ£ Process PDFs & Create Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489e843c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all PDFs\n",
    "all_pages = []\n",
    "for pdf_file in pdf_files:\n",
    "    if os.path.exists(pdf_file):\n",
    "        pages = extract_text_from_pdf(pdf_file)\n",
    "        all_pages.extend(pages)\n",
    "\n",
    "if not all_pages:\n",
    "    print(\"âš ï¸ No PDF content extracted. Please add PDF files to 'pdf_files' folder\")\n",
    "else:\n",
    "    print(f\"\\nğŸ“Š Total pages extracted: {len(all_pages)}\")\n",
    "    \n",
    "    # Create documents\n",
    "    documents = create_documents(all_pages)\n",
    "    \n",
    "    # Create vector store\n",
    "    vector_store = create_vector_store(documents, embeddings)\n",
    "    \n",
    "    print(\"\\nğŸ‰ PDF processing complete! Ready for hybrid search.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b28c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ğŸ”Ÿ Hybrid Search Function\n",
    "\n",
    "Fungsi utama yang menggabungkan PDF + Database search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e50202",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_search(question: str, include_db: bool = True) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Perform hybrid search across PDF documents and database.\n",
    "    \n",
    "    Args:\n",
    "        question: User's question\n",
    "        include_db: Whether to include database search\n",
    "        \n",
    "    Returns:\n",
    "        Dict with pdf_documents, database_results, and analysis\n",
    "    \"\"\"\n",
    "    # Analyze question\n",
    "    analysis = analyze_question_type(question)\n",
    "    search_terms = analysis['search_terms']\n",
    "    \n",
    "    print(f\"\\nğŸ” Question Analysis:\")\n",
    "    print(f\"   Type: {analysis['recommended_type'].value}\")\n",
    "    print(f\"   Is DB question: {analysis['is_db_question']}\")\n",
    "    print(f\"   Is PDF question: {analysis['is_pdf_question']}\")\n",
    "    print(f\"   Search terms: {search_terms}\")\n",
    "    \n",
    "    # Search PDFs\n",
    "    pdf_docs = []\n",
    "    if 'vector_store' in globals() and vector_store is not None:\n",
    "        pdf_docs = search_documents(vector_store, question, top_k=config.TOP_K)\n",
    "    \n",
    "    # Search Database\n",
    "    db_results = {}\n",
    "    if include_db and db_manager.conn:\n",
    "        db_results = db_manager.query(search_terms)\n",
    "        if db_results:\n",
    "            print(f\"ğŸ—„ï¸ Found database results in: {list(db_results.keys())}\")\n",
    "    \n",
    "    return {\n",
    "        \"pdf_documents\": pdf_docs,\n",
    "        \"database_results\": db_results,\n",
    "        \"analysis\": analysis,\n",
    "        \"search_terms\": search_terms\n",
    "    }\n",
    "\n",
    "\n",
    "def ask(question: str, include_db: bool = True) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Main function to ask questions - DocuLens style!\n",
    "    \n",
    "    Args:\n",
    "        question: Your question in any language\n",
    "        include_db: Include database search (default: True)\n",
    "        \n",
    "    Returns:\n",
    "        Dict with answer, sources, and metadata\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"â“ Question: {question}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    # Perform hybrid search\n",
    "    results = hybrid_search(question, include_db)\n",
    "    \n",
    "    # Generate answer\n",
    "    answer = generate_answer(\n",
    "        llm, \n",
    "        results['pdf_documents'], \n",
    "        question, \n",
    "        results['database_results']\n",
    "    )\n",
    "    \n",
    "    # Prepare sources\n",
    "    sources = []\n",
    "    for doc in results['pdf_documents']:\n",
    "        sources.append({\n",
    "            'type': 'PDF',\n",
    "            'file': doc.metadata.get('source', 'Unknown'),\n",
    "            'page': doc.metadata.get('page', '?'),\n",
    "            'score': doc.metadata.get('similarity_score', 0),\n",
    "            'preview': doc.page_content[:100] + \"...\"\n",
    "        })\n",
    "    \n",
    "    for table, rows in results['database_results'].items():\n",
    "        sources.append({\n",
    "            'type': 'Database',\n",
    "            'table': table,\n",
    "            'count': len(rows)\n",
    "        })\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\nğŸ’¬ Answer:\\n{answer}\")\n",
    "    print(f\"\\nğŸ“š Sources ({len(sources)}):\")\n",
    "    for i, src in enumerate(sources, 1):\n",
    "        if src['type'] == 'PDF':\n",
    "            print(f\"   {i}. ğŸ“„ {src['file']} (Page {src['page']}) - Score: {src['score']:.2f}\")\n",
    "        else:\n",
    "            print(f\"   {i}. ğŸ—„ï¸ Table: {src['table']} ({src['count']} results)\")\n",
    "    \n",
    "    print(f\"\\nğŸ”„ Search Type: {results['analysis']['recommended_type'].value}\")\n",
    "    \n",
    "    return {\n",
    "        'question': question,\n",
    "        'answer': answer,\n",
    "        'sources': sources,\n",
    "        'search_type': results['analysis']['recommended_type'].value\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"âœ… Hybrid search functions ready!\")\n",
    "print(\"\\nğŸ“– Usage:\")\n",
    "print('   result = ask(\"Apa isi dokumen ini?\")')\n",
    "print('   result = ask(\"Siapa saja karyawan di IT?\", include_db=True)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0cf7ac",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£1ï¸âƒ£ Test Hybrid Search! ğŸ¯\n",
    "\n",
    "Sekarang coba berbagai jenis pertanyaan:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54da5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: PDF Question - tentang isi SOP\n",
    "result = ask(\"Apa saja tahapan dalam SDLC menurut dokumen SOP?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e90e0e",
   "metadata": {},
   "source": [
    "# Test 2: Database Question (searches user_profiles table)\n",
    "result = ask(\"Siapa saja karyawan di departemen IT?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22bdfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: Database Question - cari data karyawan IT\n",
    "result = ask(\"Siapa IT Manager di perusahaan dan bagaimana cara menghubungi dia?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd8d91b",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£2ï¸âƒ£ Interactive Mode\n",
    "\n",
    "Gunakan loop interaktif untuk bertanya terus-menerus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335c038b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 3: Hybrid Question - gabungan PDF + Database\n",
    "result = ask(\"Berapa response time untuk severity Critical dan siapa yang harus dihubungi?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8170720",
   "metadata": {},
   "source": [
    "## ğŸ“ Contoh Pertanyaan Lainnya (Sesuai Schema Sekarang)\n",
    "\n",
    "Coba pertanyaan-pertanyaan berikut:\n",
    "\n",
    "**PDF Questions:**\n",
    "- \"Apa saja jenis informasi yang dijelaskan di dokumen ini?\"\n",
    "- \"Bagaimana prosedur atau kebijakan yang tertulis di PDF?\"\n",
    "- \"Ringkas isi dokumen dalam beberapa kalimat.\"\n",
    "\n",
    "**Database Questions (user_profiles / products / orders):**\n",
    "- \"Siapa saja karyawan di departemen IT?\"\n",
    "- \"Produk apa saja yang termasuk kategori Electronics?\"\n",
    "- \"Berapa total amount dan status pesanan untuk Laptop ThinkPad X1?\"\n",
    "- \"Tampilkan pesanan yang statusnya completed.\"\n",
    "\n",
    "**Hybrid Questions (PDF + Database):**\n",
    "- \"Jelaskan kebijakan di dokumen dan siapa PIC di departemen IT yang relevan.\"\n",
    "- \"Produk apa yang paling relevan dengan prosedur di dokumen ini dan siapa user yang pernah memesannya?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111ec510",
   "metadata": {},
   "source": [
    "## ğŸ“Š Advanced: View Data Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848f6a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View document chunks\n",
    "if 'documents' in globals():\n",
    "    print(f\"ğŸ“ Total document chunks: {len(documents)}\")\n",
    "    print(\"\\nFirst 3 chunks:\")\n",
    "    print(\"-\" * 50)\n",
    "    for i, doc in enumerate(documents[:3]):\n",
    "        print(f\"\\n[Chunk {i+1}]\")\n",
    "        print(f\"Source: {doc.metadata.get('source', 'Unknown')}\")\n",
    "        print(f\"Page: {doc.metadata.get('page', '?')}\")\n",
    "        print(f\"Content: {doc.page_content[:150]}...\")\n",
    "\n",
    "# View database tables\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"ğŸ—„ï¸ Database Tables:\")\n",
    "for table in db_manager.get_all_tables():\n",
    "    cursor = db_manager.conn.cursor()\n",
    "    cursor.execute(f\"SELECT COUNT(*) FROM {table}\")\n",
    "    count = cursor.fetchone()[0]\n",
    "    print(f\"   - {table}: {count} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f854f23",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“‹ Flow Pembuktian End-to-End\n",
    "\n",
    "### âœ… Fitur yang Diimplementasi:\n",
    "\n",
    "| No | Fitur | Status | Cell |\n",
    "|----|-------|--------|------|\n",
    "| 1 | Install Dependencies | âœ… | Cell 2 |\n",
    "| 2 | Import Libraries + SearchType Enum | âœ… | Cell 4 |\n",
    "| 3 | Configuration (Model, Chunk, Threshold) | âœ… | Cell 6 |\n",
    "| 4 | Load LLM Model (flan-t5) | âœ… | Cell 8, 9 |\n",
    "| 5 | PDF Extraction (pdfplumber) | âœ… | Cell 11 |\n",
    "| 6 | Document Chunking (RecursiveTextSplitter) | âœ… | Cell 11 |\n",
    "| 7 | Query Expansion (Indonesian/English) | âœ… | Cell 11 |\n",
    "| 8 | Indonesian Stemming | âœ… | Cell 11 |\n",
    "| 9 | Database Manager (SQLite) | âœ… | Cell 15 |\n",
    "| 10 | Smart Table Routing | âœ… | Cell 13 |\n",
    "| 11 | Question Type Analysis | âœ… | Cell 13 |\n",
    "| 12 | PDF Semantic Search (FAISS) | âœ… | Cell 13 |\n",
    "| 13 | Hybrid Search (PDF + DB) | âœ… | Cell 20 |\n",
    "| 14 | Answer Generation | âœ… | Cell 13 |\n",
    "| 15 | Interactive Q&A | âœ… | Cell 24 |\n",
    "\n",
    "### ğŸ”„ Flow Eksekusi:\n",
    "\n",
    "```\n",
    "[Cell 2] Install Dependencies\n",
    "    â†“\n",
    "[Cell 4] Import Libraries\n",
    "    â†“\n",
    "[Cell 6] Load Configuration\n",
    "    â†“\n",
    "[Cell 8-9] Load LLM & Embeddings\n",
    "    â†“\n",
    "[Cell 11] Define PDF Processing Functions\n",
    "    â†“\n",
    "[Cell 13] Define Search & Answer Functions\n",
    "    â†“\n",
    "[Cell 15] Setup Database (Optional)\n",
    "    â†“\n",
    "[Cell 17] Load PDF Files (from folder)\n",
    "    â†“\n",
    "[Cell 19] Process PDFs â†’ Create Vector Store\n",
    "    â†“\n",
    "[Cell 20] Define Hybrid Search Function\n",
    "    â†“\n",
    "[Cell 22-24] Test Questions:\n",
    "    - PDF: \"Jelaskan isi dokumen ini\"\n",
    "    - DB: \"Siapa karyawan di IT?\"\n",
    "    - Hybrid: \"Harga laptop dan spesifikasi\"\n",
    "    â†“\n",
    "[Cell 24] Interactive Mode\n",
    "```\n",
    "\n",
    "### ğŸ“ Cara Menambah PDF:\n",
    "\n",
    "1. Buat folder `pdf_files` di Colab\n",
    "2. Upload PDF ke folder tersebut\n",
    "3. Re-run Cell 17 dan 19\n",
    "\n",
    "### ğŸ—„ï¸ Cara Menambah Data Database:\n",
    "\n",
    "Edit Cell 15 untuk menambah tabel/data sesuai kebutuhan.\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ”— Full API Version:** [DocuLens on HuggingFace](https://huggingface.co/spaces/krisnasetyadi/doculens-api)\n",
    "\n",
    "**ğŸ’» Frontend:** [DocuLens UI](https://github.com/krisnasetyadi/doculens-ui)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
